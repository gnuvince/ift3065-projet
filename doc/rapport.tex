\documentclass[11pt]{report}

\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{multicol}
\usepackage{url}
\usepackage{syntax}

\begin{document}

\newcommand{\sins}{\emph{SINS}}
\newcommand{\bop}{\textbf{(}}
\newcommand{\bcp}{\textbf{)}}

\title{IFT3065 - Rapport sur SINS}
\author{Vincent Foley-Bourgon (FOLV08078309) \\
Eric Thivierge (THIE09016601)}
\maketitle

\abstract

\sins{} (Scheme IN Scheme) est un compilateur Scheme écrit dans le
langage Scheme dans le cadre du cours IFT3065 de la session hiver 2012
à l'Université de Montréal.  Ce rapport décrit la philosophie
directrice du projet, les différentes parties du compilateur, les
problèmes et difficultés rencontrés durant son écriture et les
solutions mises en place pour résoudre ces problèmes.

\chapter{SINS}

\section{Qu'est-ce que SINS?}

\sins{} (Scheme IN Scheme) est un petit compilateur Scheme écrit
dans le cadre d'un cours de compilation de 15 semaines.  Comme son nom
l'indique, il s'agit d'un compilateur pour Scheme écrit en Scheme.
\sins génère du code assembleur x86.

\sins{} est un véhicule permettant à ses auteurs de faire un
survol des différents aspects de la compilation.  La philosophie de
base est qu'étant donné le choix d'implanter une fonctionnalité en
profondeur ou d'en implanter deux avec des techniques moins
sophistiquées, on choisira la deuxième option à tout coup.  Nous
sommes d'avis que cette approche nous donnera une vision plus globale
d'un compilateur.  Les détails d'implantation plus avancés pourront
être appris, implantés et maîtrisés dans le cadre de stages, de
projets personnels, de recherches et d'emplois.


\section{Design de SINS}

\sins{} suit un modèle en pipeline où chaque étape de la compilation a
son propre module et où chaque module a comme entrée la sortie du
module précédent.  Chaque module effectue (si possible) une seule
transformation afin de garder la complexité du module la plus basse
possible.  Cela implique que \sins{} fait plus de passes sur le code et
qu'il est plus lent à l'exécution qu'un compilateur qui fusionne
plusieurs phases en une.  Dans le cadre d'un projet éducationnel,
nous jugeons que cette contrainte est acceptable.

\section{Milestone du 12 mars 2012}

En date du 12 mars 2012,  les nouvelles fonctionnalités suivantes sont
implantées:
\begin{itemize}
\item Le parseur traite toutes les formes de notre sous-ensemble de Scheme
\item Le parseur retourne un arbre de syntaxe abstraite
%\item * Expansion de macro
\item Le langage Scheme d'entrée est simplifié à un langage Scheme
  utilisant moins de constructions
\item Plus d'une centaine de tests exercent les différents modules de \sins{}
\item Un makefile permet d'exécuter automatiquement la suite de tests
\end{itemize}


\section{Milestone du 13 février 2012}

En date du 13 février 2012, les fonctionnalités suivantes sont
implantées:

\begin{itemize}
\item L'analyse lexicale
\item Une suite de tests unitaires pour l'analyse lexicale
\item Le lecteur
\item La vérification syntaxique des formes spéciales
\end{itemize}

Notre code se trouve sur le site GitHub, et le code en date du 13
février 2012 peut être visionné à l'adresse suivante:
\url{https://github.com/gnuvince/ift3065-projet/tags}.


\chapter{Analyse lexicale}

La première phase habituelle d'un compilateur est de transformer un
flux de caractères représentant un programme source en un flux de
lexèmes.  Dans ce flux sortant, les espaces blancs et les commentaires
sont éliminés, et les caractères représentant des nombres ou des mots
sont regroupés ensemble pour une analyse syntaxique plus facile.

Il existe de nombreux outils automatisés qui permettent de faire
l'analyse lexicale; \emph{lex}, \emph{flex} et \emph{silex} sont de
tels outils.  Malgré leur disponibilité, nous avons décidé de ne pas
utiliser ces outils et de plutôt créer notre propre analyseur lexical.
Cette décision a été prise afin de mieux comprendre les détails de
plus bas niveau, ainsi que les difficultés d'implantation de cette
phase.

La grammaire reconnue par l'analyse lexicale de \sins{} est décrite
dans la section \ref{grammar-tokens}.

\section{Structures de données}

Afin de faire l'analyse lexicale d'un programme source, deux
structures de données principales ont été nécessaires: un type flux
(\emph{stream}), et un type lexème (\emph{token}).

\subsection{Stream}

La fonction \emph{make-stream} prend en entrée une chaîne de
caractères et retourne un \emph{stream}.  La fonction
\emph{make-stream-from-port} prend en entrée un port d'entrée et
retourne un \emph{stream}.  Un \emph{stream} est une structure de
données possédant les 4 opérations suivantes:

\begin{itemize}
\item {\bf next}: cette fonction retourne le caractère courant du
  flux.  Si le flux est terminé, elle retourne le caractère nul.
\item {\bf advance}: cette fonction fait avancer le curseur d'un
  caractère dans le flux et met à jour les attributs \emph{line} et
  \emph{col}.  \emph{Advance} n'a pas de valeur de retour.
\item {\bf line}: retourne la ligne courante.
\item {\bf col}: retourne la colonne courante.
\end{itemize}

\subsection{Token}

Un lexème est représenté par une liste ayant la forme suivante:

\begin{verbatim}
(token <(type . valeur)> <ligne> <colonne>)
\end{verbatim}

Le premier élément est une étiquette qui permet de distinguer un
lexème d'un autre type de données.  La paire en deuxième position
donne l'information sur le type du lexème (ex.: mot-clé,
identificateur, etc.) et la valeur qui lui est associée.  Les
troisième et quatrième champs conservent la position du début du
lexème.

Une erreur dans un lexème (par exemple: constante de caractère
invalide) est représentée par une liste ayant la forme suivante:

\begin{verbatim}
(token-error <ligne> <colonne>)
\end{verbatim}


On peut manipuler les lexèmes à l'aide des fonctions suivantes:

\begin{itemize}
\item {\bf make-token}: cette fonction permet de créer un lexème en
  donnant son type et sa valeur, sa ligne, et sa colonne.  Si
  type/valeur est \#f, \emph{make-token} retourne un erreur de lexème.
\item {\bf token?}: un prédicat qui détermine si un objet représente
  un lexème.
\item {\bf token-error?}: un prédicat qui détermine si un objet
  représente une erreur dans un lexème.
\item {\bf token-type}: retourne le type du lexème.
\item {\bf token-value}: retourne la valeur du lexème.
\item {\bf token-symbol}: retourne le type et la valeur d'un lexème.
\item {\bf token-line}: retourne la ligne où commence ce lexème.
\item {\bf token-col}: retourne la colonne où commence ce lexème.
\end{itemize}

\section{Algorithme d'analyse}

La fonction \emph{get-token} prend en entrée un flux de caractères et
retourne le prochain lexème.  Voici une explication de son
fonctionnement.

Tout d'abord, \emph{get-token} commence par ignorer les espaces blancs
et les commentaires.  Une fois que le flux se trouve sur un caractère
non-blanc, on effectue une analyse de cas.  Si le caractère courant
est un des terminaux suivants du langage, on retourne un lexème
représentant ce symbole: \texttt{(}, \texttt{)}, \texttt{,},
\texttt{,@}, \texttt{'}, \texttt{`}, et le caractère nul.

Dans le cas où le caractère est une guillemet, on accumule les
caractères du flux jusqu'à ce qu'on consomme la guillemet fermante (en
faisant attention aux séquences d'échappement).

Dans le cas où le caractère est un dièse, on essaye de lire, en ordre,
la constante vraie (\#t), la constante fausse (\#f) ou un caractère
(\#\textbackslash).  Dans le cas du caractère, si le caractère suivant
le backslash n'est pas alphabétique, on retourne immédiatement le code
ASCII de ce caractère.  Si le caractère est alphabétique, on tente de
lire un nom de caractère.  Nous supportons les noms ``nul'', ``tab'',
``space'' et ``newline''.  Si le nom possède un seul caractère, on
retourne le code ASCII associé à ce caractère.  Dans tous les autres
cas, on retourne \#f pour signaler une erreur.

Si le caractère est un chiffre, on lit un nombre.  Cela empêche des
identificateurs débutant par un chiffre comme \emph{2pi} par exemple.
Cette limitation est acceptable dans notre compilateur (et est
conforme avec la syntaxe décrite dans la section 7.1.1 du standard
R5RS).

Dans le cas où le caractère est un caractère qui fait partie de
l'alphabet des identificateurs, on procède en deux étapes:

\begin{enumerate}
\item On lit tous les caractères identificateurs;
\item On classifie le symbole et construisons le lexème approprié.
\end{enumerate}

Si le symbole représente un mot clé, on retourne un lexème de type
\emph{keyword}; autrement, on retourne un lexème de type
identificateur.

Finalement, pour tous les autres caractères, on retourne une erreur de
lexème.

La fonction \emph{lex} permet de prendre en entrée un flux de
caractères et de retourner une liste de lexèmes.  Les fonctions
\emph{lex-from-string} et \emph{lex-from-file} créent un flux à partir
d'une chaîne de caractères et d'un fichier source respectivement et
font ensuite appel à \emph{lex}.


\chapter{Analyse syntaxique}

Tout comme l'analyse lexicale, il existe de nombreux outils pour faire
l'analyse syntaxique d'un flux de lexèmes; Yacc, Bison, LALR-SCM et
ANTLR sont de tels outils.  Et tout comme pour la phase d'analyse
lexicale, afin de mieux comprendre les détails d'implantation de cette
phase et les difficultés qui y sont associées, nous avons décidé de ne
pas utiliser un de ces outils et de plutôt créer notre propre module
d'analyse syntaxique.

Comme la syntaxe de Scheme est simple et peut être exprimée par une
grammaire LL(1), il est naturel de créer cette phase par descente
récursive.  De plus, comme la récursion est omniprésente en Scheme et
que le langage possède des symboles, Scheme est un très bon choix pour
faire cette phase.

Les sections suivantes décriront les différentes phases de l'analyse
syntaxique, les structures de données utilisées ainsi que certains
détails d'implantation que nous jugeons intéressants.

La grammaire reconnue par \sins{} est décrite dans l'annexe
\ref{grammaire}.


\section{Le lecteur}

Une caractéristique propre aux langages Lisp est le lecteur
(\emph{reader} en anglais); le lecteur prend en entrée une chaîne de
caractères représentant une structure Lisp et retourne la
représentation arborescente de cette structure.

Dans cette phase, les symboles sont auto-évalués (et non pas traités
comme des variables) et les listes sont vues non pas comme des appels
de fonctions ou des formes spéciales, mais simplement comme des listes
de symboles.  Voici un exemple d'interaction avec la fonction
\emph{read} dans l'interpréteur de Gambit:

\begin{verbatim}
> (read (open-input-string "allo"))
allo
> (read (open-input-string "(lambda)"))
(lambda)
> (read (open-input-string "(sins est un (bon) compilateur)"))
(sins est un (bon) compilateur)
\end{verbatim}

Les seules vérifications syntaxiques faites par le lecteur sont de s'assurer
que le parenthèsage des expressions est correct et que les listes (propres et
impropres) sont bien formées.  Aucune vérification syntaxique plus approfondie
n'est faite (ex.: qu'une forme lambda soit suivie d'une liste de paramètres
puis d'expressions à exécuter).  Ceci est nécessaire pour la prochaine phase
de l'analyse syntaxique, l'expansion des macros.

Après l'analyse lexicale nous avons une structure de lexèmes; après la phase
de lecture des lexèmes, nous avons une structure arborescente sémantiquement
équivalente au programme source. Nous appelons cette structure arbre de
syntaxe abstraite (AST).

\subsection{Structures de données}

Tout comme l'analyse lexicale, le lecteur possède une flux de
données.  Cette fois, ce sont des lexèmes qui passent sur le flux de
données.  Les opérations suivantes sont supportées:

\begin{itemize}
\item {\bf empty}: retourne vrai ou faux si le flux est vide ou terminé.
\item {\bf next}: retourne le prochain lexème sans modifier le flux.
  Si le flux est vide, la liste vide est retournée.
\item {\bf advance}: consomme le lexème actuel et déplace le flux au
  prochain lexème.  Aucune donnée n'est retournée.
\item {\bf pop}: consomme le lexème actuelle et déplace le flux au
  prochain lexème.  Le lexème consommé est retourné.
\end{itemize}

\section{L'expansion de macros}

Le code d'expansion de macro fourni en classe n'a pas encore été complétement
intégré à \sins{}.

\section{Vérification de l'AST}

Tandis que le lecteur fait une vérification de la forme externe du programme
(le parenthèsage), le parseur effectue la vérification des formes internes du
programme. Par exemple, le parseur vérifie qu'une lambda expression est formée
du mot clé ``lambda'', suivi soit d'un identificateur, soit d'une liste
d'identificateurs (possiblement vide), suivi d'une ou plusieurs expressions.

\subsection{Structures de données}

\sins{} représente l'AST sous forme de listes imbriquées de symboles. Nous
avons choisi cette représentation pour sa simplicité, son faible coût en
mémoire et la facilité de manipulation de cette structure en Scheme.

Afin de garder l'implantation du compilateur plus simple, nous avons
décidé de ne pas inclure les informations relatives aux lignes et
colonnes dans l'AST. Cette décision donnera des messages d'erreur
moins précis, mais nous jugeons que la simplicité gagnée par ce
compromis est plus importante dans le cadre d'un projet pédagogique.
Si le temps le permet, nous pourrons considérer revoir cette décision.

\subsection{Algorithmes}

Nous avons choisi une forme déclarative pour le parseur où chaque fonction
correspond à une production de la grammaire de Scheme (R5RS). La fonction
principale \emph{parse} reçoit une liste d'expressions formant le programme à
vérifier.

Chaque fonction représentant un production reçoit en paramètre un AST à
vérifier et teste séquentiellement les alternatives de cette production en
relation avec l'AST. Si l'une des production unifie l'AST, la fonction retourne
l'AST. Sinon elle retourne faux.

Cette forme déclarative pour le parseur permet d'exprimer une structure de
code claquée sur les productions de la grammaire ce qui donne un programme
facile à valider et à maintenir.


\section{Simplification}

Afin de diminuer l'effort de génération de code, nous avons implanté
un module de simplification ({\tt src/ir/simplification.scm}). Ce
module prend l'AST du programme d'entrée et retourne un AST dans
lequel nous avons substitué des formes à des formes plus simples. La
procédure de simplification est appliquée récursivement jusqu'à ce
qu'il ne reste que des formes simplifiées.

Voici les différentes simplifications qui sont faites en date de la
remise du 12 mars 2012.

\subsection{\tt begin}

La forme {\tt (begin $e_1$ $e_2$ ... $e_n$)} est remplacée par une
série de {\tt let} imbriqués.


\begin{verbatim}
(begin e1) => e1
(begin e1 e2) => (let ((:g1 e1)) e2)
(begin e1 e2 e3) => (let ((:g1 e1)) (let ((:g2 e2)) e3))
\end{verbatim}

(Les symboles {\tt :g1}, etc. sont générés automatiquement et sont
uniques.)

\subsection{\tt let}

La forme {\tt let} sans étiquette est transformée en une expression
lambda.

\begin{verbatim}
(let () e1) => e1
(let ((a x)) e1) => ((lambda (a) e1) x)
(let ((a x) (b y)) e1) => ((lambda (a b) e1) x y)
\end{verbatim}

La forme {\tt let} avec étiquette est transformée en une forme {\tt
  letrec} (nous verrons que plus tard, {\tt letrec} est transformée
en {\tt let} sans étiquette).

\begin{verbatim}
(let t () e1) => (letrec ((t (lambda () e1))) (t))
(let t ((a x)) e1) => (letrec ((t (lambda (a) e1))) (t x))
\end{verbatim}


\subsection{\tt let*}

La forme {\tt let*} est transformée en une imbrication de {\tt let}.

\begin{verbatim}
(let* () e1) => e1
(let* ((a x)) e1) => (let ((a x)) e1)
(let* ((a x) (b a)) e1) => (let ((a x)) (let ((b a)) e1))
\end{verbatim}


\subsection{\tt letrec}

La forme {\tt letrec} est transformée en un {\tt let} où on modifie la
valeurs des variables dans le corps du {\tt let}.

\begin{verbatim}
(letrec () e1) => e1
(letrec ((f x)) e1) => (let ((f #f)) (set! f x) e1)
(letrec ((f x) (g y)) e1) =>
   (let ((f #f) (g #f)) (set! f x) (set! g y) e1)
\end{verbatim}


\subsection{\tt lambda}

Le corps d'une forme {\tt lambda} est transformé en un {\tt begin}.

\begin{verbatim}
(lambda () e1) => (lambda () e1)
(lambda () e1 e2) => (lambda () (begin e1 e2))
\end{verbatim}


\subsection{\tt cond}

La forme {\tt cond} est transformée en une imbrication de {\tt if}.


\begin{verbatim}
(cond (else 3)) => 3
(cond (p1 1) (else 3)) => (if p1 1 3)
(cond (p1 1) (p2 2) (else 3)) => (if p1 1 (if p2 2 3))
\end{verbatim}


\subsection{\tt case}

La forme {\tt case} est transformée en une imbrication de {\tt if} et
de {\tt memv}. L'expression sur laquelle on fait le test est évaluée
une seule fois et son résultat est sauvegardé.  Cela évite de faire
des opérations avec effets plusieurs fois.


\begin{verbatim}
(case x (else 3)) => 3
(case x ((1) 1) (else 3)) =>
   (let ((:g1 x)) (if (memv x (1)) 1 3))
(case x ((1) 1) ((2) 2) (else 3)) =>
   (let ((:g1 x))
     (if (memv :g1 (1))
         1
         (let ((:g2 :g1))
           (if (memv :g2 (2))
               2
               3))))
\end{verbatim}

\subsection{\tt or}

La forme {\tt or} est transformée en une imbrication de {\tt if}; le
résultat de chaque expression est sauvegardé dans une variable afin
d'éviter d'évaluer plusieurs fois les expressions.


\begin{verbatim}
(or) => #f
(or e1) => e1
(or e1 e2) => (let ((:g1 e1)) (if :g1 :g1 e2))
\end{verbatim}

\subsection{\tt and}

La forme {\tt and} est transformée en une imbrication de {\tt if}.


\begin{verbatim}
(and) => #t
(and e1) => e1
(and e1 e2) => (if e1 (if e2 e2 #f) #f)
\end{verbatim}



\chapter{Environnement}

Dans un compilateur, il est nécessaire de faire la gestion de
l'environnement, des symboles qui sont accessibles ainsi que la façon
de les trouver.  Dans \sins{}, la gestion de l'environnement est
faite dans le module {\it backend/env.scm}.

\section{Structures de données}

L'environnement est représentée par une liste deux éléments. Le
premier élément contient la distance en octets entre le prochain
symbole local et le début du frame.  Le second élément est une liste
de tuples contenant 3 informations:


\begin{verbatim}
(<symbole> <portée> <offset>)
\end{verbatim}

Dans le cas d'une variable locale ou d'un paramètre de fonction, la
portée est représentée par le symbole {\it local} et l'offset est la
distance entre le début du frame et la valeur du symbole.

Dans le cas d'une variable capturée par une fermeture, la portée est
représentée par le symbole {\it captured} et l'offset est l'index de
la valeur du symbole dans la fermeture.

Le module d'environnement fournit plusieurs fonctions pour manipuler
un environnement.

\begin{itemize}
\item {\bf make-env}: crée et retourne un environnement vide.
\item {\bf env-fs}: retourne la valeur du frame size (premier élément
  de l'environnement).
\item {\bf env-symbols}: retourne la liste des symboles (second
  élément de l'environnement).
\item {\bf env-fs++}: augmente d'un mot la valeur du frame size et
  retourne le nouvel environnement modifié.
\item {\bf env-fs+}: augmente d'un nombre spécifique de mots la valeur
  du frame size et retourne le nouvel environnement modifié.
\item {\bf env-add-symbol}: ajoute un symbole à la liste de symbole,
  incrémente la valeur du frame size, et retourne le nouvel
  environnement modifié.
\item {\bf env-lookup}: tente de retourner le tuple d'un symbole.  Si
  aucun tuple local ou capturé est trouvé, on retourne le tuple d'une
  variable globale.
\item {\bf env-update}: fonction utilitaire qui ajoute une liste de
  symboles locaux dans l'environnement.
\end{itemize}



\chapter{Problèmes et solutions}

\section{Analyse lexicale}

L'analyse lexicale a probablement été la partie la plus facile à
compléter. L'implantation suit de très près la théorie sur les
automates à états finis et a été complétée et testée sans anicroche.

\section{Analyse syntaxique}

Une difficulté rencontrée dans l'analyse syntaxique a été la gestion
de la valeur fausse (\#f).  En effet, il était initialement impossible
de distinguer entre la valeur \#f lue lors de l'analyse lexicale et la
valeur logique fausse.  Ainsi, un code comme ceci pouvait causer
problème:


\begin{verbatim}
(define (<literal> ast)
  (and (or (<quotation> ast)
           (<self-evaluating> ast))
       ast))
\end{verbatim}

Cette fonction est utilisée pour lire un litéral Scheme, dont \#f fait
partie.  Cette fonction doit retourner l'arbre de syntaxe abstraite,
mais comme celui-ci est \#f la fonction appelante n'a aucune façon de
savoir si {\tt <literal>} a retourné \#f parce que c'est le litéral
qui a été lu ou parce qu'il y a une erreur.

Afin de régler ce problème, nous avons procédé en 2 étapes:

\begin{enumerate}
\item Dans la phase d'analyse lexicale, nous avons remplacé le lexème
  pour la valeur fausse de {\tt (boolean . #f)} à {\tt ('boolean .
    false)} où {\tt false} est un symbole unique généré par le
  compilateur.
\item Dans la phase d'analyse syntaxique, lorsque l'arbre est généré,
  nous le parcourons et transformons toutes les feuilles {\tt false}
  par \#f.
\end{enumerate}



\appendix
\chapter{Grammaire}
\label{grammaire}
Voici une description EBNF de la grammaire supportée par \sins{}. Au
moment d'écrire le rapport de la première remise, \sins{} ne
supportait pas encore les abbréviations complétement mais nous avons
choisi de l'inclure dans la description malgré tout. Cette grammaire
évoluera probablement au cours du projet.

\setlength{\grammarindent}{3em}


\section{Lexèmes}
\label{grammar-tokens}
\begin{grammar}
  <alpha> $\longrightarrow$ `a..zA..Z'

  <digit> $\longrightarrow$ `0..9'

  <boolean> $\longrightarrow$ `\#f' | `\#t'

  <extended> $\longrightarrow$ `!' | `\$' | `\%' | `\&' | `*' | `+' | `-' | `.' | `/' | `:'
  | `<' | `=' | `>' | `?' | `@' | `\textasciicircum' | `\_' | `\textasciitilde'

  <number> $\longrightarrow$ <digit> <digit>*

  <identifier> $\longrightarrow$ (<alpha> | <extended>) (<alpha> | <extended> |
  <digit>)*

  <keyword> $\longrightarrow$ `define' | `else' | `unquote' | `unquote-splicing' | `quote'
  \alt `lambda' | `if' | `set!' | `begin' | `cond' | `and' | `or'
  \alt `case' | `let' | `let*' | `letrec' | `do' | `delay' |
  `quasiquote'

  <string> $\longrightarrow$ `\textquotedbl' <characters>* `\textquotedbl'

  <character> $\longrightarrow$ `#\textbackslash' ( <any character> | `nul' |
  `newline' | `space' | `tab' )

\end{grammar}



\section{datum}
\begin{grammar}
  <datum> $\longrightarrow$ <simple datum> \alt <compound datum>

  <simple datum> $\longrightarrow$ <boolean> \alt <number>
  \alt <character> \alt <string> \alt <symbol>

  <symbol> $\longrightarrow$ <identifier>

  <compound datum> $\longrightarrow$ <list>

  <list> $\longrightarrow$ \bop <datum>* \bcp \alt \bop <datum>+  .  <datum> \bcp
  \alt <abbreviation>

  <abbreviation> $\longrightarrow$ <abbrev prefix> <datum>

  <abbrev prefix> $\longrightarrow$ \'{} \alt \`{} \alt , \alt ,@

\end{grammar}

\section{program}

\begin{grammar}
  <program> $\longrightarrow$ <command or definition>*

  <command or definition> $\longrightarrow$ <command>
  \alt <definition>
  \alt \bop \textbf{begin} <command or definition>+ \bcp

  <definition> $\longrightarrow$ \bop define <variable> <expression> \bcp
  \alt \bop \textbf{define} \bop <variable> <def formals> \bcp <body> \bcp
  \alt \bop \textbf{begin} <definition>* \bcp

  <def formals> $\longrightarrow$ <variable>*
  \alt <variable>*  .  <variable>
\end{grammar}

\section{expression}

\begin{grammar}
  <expression> $\longrightarrow$ <variable>
  \alt <literal>
  \alt <procedure call>
  \alt <lambda expression>
  \alt <conditional>
  \alt <assignment>
  \alt <derived expression>

  <literal> $\longrightarrow$ <quotation> \alt <self-evaluating>

  <self-evaluating> $\longrightarrow$ <boolean> \alt <number>
  \alt <character> \alt <string>

  <quotation> $\longrightarrow$ '<datum> \alt \bop quote <datum> \bcp

  <procedure call> $\longrightarrow$ \bop <operator> <operand>* \bcp

  <operator> $\longrightarrow$ <expression>

  <operand> $\longrightarrow$ <expression>

  <lambda expression> $\longrightarrow$ \bop lambda <formals> <body> \bcp

  <formals> $\longrightarrow$ \bop <variable>* \bcp \alt <variable>
  \alt \bop <variable>+  .  <variable> \bcp

  <body> $\longrightarrow$ <definition>* <sequence>

  <sequence> $\longrightarrow$ <command>* <expression>

  <command> $\longrightarrow$ <expression>

  <conditional> $\longrightarrow$ \bop \textbf{if} <test> <consequent> <alternate> \bcp

  <test> $\longrightarrow$ <expression>

  <consequent> $\longrightarrow$ <expression>

  <alternate> $\longrightarrow$ <expression> \alt <empty>

  <assignment> $\longrightarrow$ \bop \textbf{set!} <variable> <expression> \bcp

  <derived expression> $\longrightarrow$ \bop cond <cond clause>+ \bcp
  \alt \bop \textbf{cond} <cond clause>* \bop else <sequence> \bcp \bcp
  \alt \bop \textbf{and} <test>* \bcp
  \alt \bop \textbf{or} <test>* \bcp
  \alt \bop \textbf{let} \bop <binding spec>* \bcp <body> \bcp
  \alt \bop \textbf{let} <variable> \bop <binding spec>* \bcp <body> \bcp
  \alt \bop \textbf{let*} \bop <binding spec>* \bcp <body> \bcp
  \alt \bop \textbf{letrec} \bop <binding spec>* \bcp <body> \bcp
  \alt \bop \textbf{begin} <sequence> \bcp
  \alt <quasiquotation>

  <cond clause> $\longrightarrow$ \bop <test> <sequence> \bcp
  \alt \bop <test> \bcp
  \alt \bop <test> => <recipient> \bcp

  <recipient> $\longrightarrow$ <expression>


  <binding spec> $\longrightarrow$ \bop <variable> <expression> \bcp

\end{grammar}

\section{quasiquotation}
\begin{grammar}
  <quasiquotation> $\longrightarrow$ \`{}<expression>
  \alt \bop \textbf{quasiquote} <expression> \bcp
\end{grammar}

\end{document}
